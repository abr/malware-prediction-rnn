from sklearn.metrics import f1_score, accuracy_score
from copy import deepcopy
import numpy as np
import random
import operator
from itertools import combinations
import csv
import time
import os
import gc
import sys
from .useful import *
from .RNN import generate_model

random.seed(11)
np.random.seed(11)

class Experiment():
	"""docstring for Experiment"""
	def __init__(self, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		data=None, folds=10, 
		folder_name=str(time.time()),
		thresholding=False, threshold=0.5):
		
		assert (type(folds) is int) or (data == None), "folds must be an integer if data tuple is provided"
		assert (search_algorithm.lower() == "grid") or (search_algorithm.lower() == "random"), "Only 'grid' and 'random' permissible values for search_algorithm"

		self.headers = ["Time","DB_totalpro","DB_maxPID","DB_CPUUser","DB_CPUSys","DB_memUse","DB_swapUse","DB_RXPac","DB_TXPac","DB_RXByte","DB_TXByte"]

		#hyperparameters
		self.original_h_params = parameters
		self.h_params = parameters		
		
		# set up parameter search space depending on algorithm
		self.search_algorithm = search_algorithm
		self.current_params = {}

		if self.search_algorithm == "grid":
			self.h_params = dict([(key, sorted(list(self.h_params[key]))) for key in self.h_params])
			self.original_h_params = deepcopy(self.h_params)
			self.current_params = dict([(key, self.h_params[key][0]) for key in self.h_params])
		if self.search_algorithm == "random":
			self.__list_to_dict_params() # list instances to even probability dictionary instances
			self.__map_to_0_1() # re-adjust any instances in which sum(probabilities) > 1

		# metrics and writer objects, to be assigned when first experiment written up
		self.folder_name = check_filename(folder_name)
		self.experiment_id = 0
		self.metrics_headers = None
		self.metrics_writer = None 

		#Model generator
		self.generate_model = generate_model
		
		# Thresholding set up
		self.thresholding = thresholding
		if self.thresholding:
			self.min_threshold = threshold + K.epsilon()
			self.temp_min_threshold = threshold + K.epsilon()

		# test-train experiment
		if (data == None) or (folds == None):
			self.folds = None
			self.X_TRAIN = x_train
			self.Y_TRAIN = y_train
			self.X_TEST = x_test
			self.Y_TEST = y_test
			print("Test-train experiment")
		# k-fold cross-validation experiment
		else:
			assert folds != None, "Supply number of folds for k-fold cross validation or supplt x_train, y_train, x_test, y_test"
			self.folds = folds
			self.x = data[0]
			self.y = data[1]
			print(self.folds, "- fold cross validation experiment")



	def __list_to_dict_params(self):
			for key in self.h_params:
				if type(self.h_params[key]) is list:
					self.h_params[key] = dict([(x, 1/(len(self.h_params[key]) + K.epsilon() )) for x in self.h_params[key]])

	def __map_to_0_1(self):
		"""maps input probabilities to values between 0 and 1, preserving scalar relationships"""
		for key in self.h_params:
			running_total = 0
			scalar = 1/(sum(self.h_params[key].values()))
			for possible_value in self.h_params[key]:
				if self.h_params[key][possible_value] < 0:
					raise ValueError("Negative hyperparameter probabilities are not allowed ({} for {})").format(self.h_params[key][possible_value], possible_value)
				new_value = self.h_params[key][possible_value] * scalar
				self.h_params[key][possible_value] = new_value + running_total
				running_total += new_value

	def __random_config(self):
		"""randomly generate a configuration of hyperparameters from dictionary self.h_params"""
		for key in self.h_params:
			choice = random.random()
			sorted_options = sorted(self.h_params[key].items(), key=operator.itemgetter(1))
			for option in sorted_options:
				if choice < option[1]:
					self.current_params[key] = option[0]
					print(key, ":", self.current_params[key], end=" - ")
					break
		if self.current_params["optimiser"] == "adam":
			self.current_params["learning_rate"] = 0.001
		print()

	def run_one_experiment(self):
		#Get new configuration if random search
		if self.search_algorithm == "random":
			self.__random_config()	

		self.experiment_id += 1
		print("running expt", self.experiment_id, "of", self.num_experiments)
		
		self.metrics = {}
		self.current_fold = 1
		self.accuracy_scores = []

		# k-fold cross-validation
		if self.folds != None:
			y = deepcopy(self.y)
			x = deepcopy(self.x)

			#remove short seqeunces and store indicies of kept items
			x, y, identifiers = remove_short_idx(x, y, list(range(len(y))), self.current_params["sequence_length"])
			labels = {}
			temp_y = deepcopy(y).flatten() # get labels as flat array to find stratified folds

			for i, class_label in zip(identifiers, temp_y):
				if class_label in labels:
					labels[class_label].append(i)
				else:
					labels[class_label] = [i]

			#split into number of folds
			fold_indicies = [[] for x in range(self.folds)]
			# Want to represent class distribution in each set (stratified split)
			# Divide indicies list in chunks of size folds
			for key in labels:
				labels[key] = to_chunks(labels[key], self.folds)
				for i, fold_ids in enumerate(labels[key]):
					fold_indicies[i] += fold_ids

			fold_indicies = np.array([[int(x) for x in index_set] for index_set in fold_indicies])
			#take new copies of the original to use indicies from the original sets
			x, y = deepcopy(self.x), deepcopy(self.y)

			for i in list(range(self.folds)):
				test = np.array([(i) % self.folds]) # one fold is test set
				train = np.array([i for i in range(self.folds) if i not in [test]]) # remaining folds are training set ( not in [test, val])

				test_idxs = np.concatenate(tuple([fold_indicies[i] for i in test]))
				train_idxs = np.concatenate(tuple([fold_indicies[i] for i in train]))

				self.x_train, self.y_train = truncate_and_tensor(x[train_idxs], y[train_idxs], self.current_params["sequence_length"])
				self.x_test, self.y_test = truncate_and_tensor(x[test_idxs], y[test_idxs], self.current_params["sequence_length"])

				self.test_idxs = test_idxs
				
				stop = self.set_up_model()
				if stop:
					return

				self.current_fold += 1

		# test-train
		else:
			self.x_train = deepcopy(self.X_TRAIN)
			self.y_train = deepcopy(self.Y_TRAIN)
			self.x_test = deepcopy(self.X_TEST)
			self.y_test = deepcopy(self.Y_TEST)

			self.test_idxs = np.array(range(1, len(self.y_test) + 1)) / 10 #divide by 10 to distinguish from training/10-fold data

			# remove short sequences - store indicies for test data
			self.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params["sequence_length"])
			self.x_test, self.y_test, self.test_idxs = remove_short_idx(self.x_test, self.y_test, self.test_idxs, self.current_params["sequence_length"])

			self.set_up_model()
		

	def set_up_model(self):
		# Leave out feature if specified in dictionary
		if "leave_out_feature" in self.current_params:
			print("Omitting feature:", self.headers[self.current_params["leave_out_feature"]])
			self.x_train = np.delete(self.x_train, self.current_params["leave_out_feature"], 2)
			self.x_test = np.delete(self.x_test, self.current_params["leave_out_feature"], 2)

		
		#Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
		self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
		
		#scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)

		#Output size - in future delete any cols for categorical which are all zero 
		print("train, test set size (x):", self.x_train.shape, self.x_test.shape)
		print("train, test set size (y):", self.y_train.shape, self.y_test.shape)
		model = self.generate_model(self.x_train, self.y_train, self.current_params)

		if self.current_fold == 1:
			print(model.summary())

		return self.train_model(model) #Returns TRUE if accuracy below threshold 
			

	def train_model(self, model):
		"""run one fold and write up results"""
		print("		fold ", self.current_fold, "of", self.folds)
		metrics = self.metrics
		reset_states = ResetStatesCallback()

		start_train = time.time()
		h = model.fit(
			self.x_train, self.y_train, 
			batch_size=self.current_params["batch_size"], 
			epochs=self.current_params["epochs"],
			verbose=0, 
			shuffle=True,	
			callbacks=[reset_states])

		end_train = time.time()

		metrics["train_acc"] = h.history["acc"]

		start_test = time.time()
		pred_Y = model.predict(self.x_test, batch_size=self.current_params["batch_size"])
		metrics["preds"] = [x[0] for x in pred_Y]
		end_test = time.time()
		metrics["truth"] = self.y_test.flatten()
		metrics["categorical_preds"] = [np.round(x) for x in metrics["preds"]]
		metrics["fscore"] = f1_score(metrics["truth"], metrics["categorical_preds"])
		metrics["accuracy"] = accuracy_score(metrics["truth"], metrics["categorical_preds"])		
		metrics["experiment_id"] = self.experiment_id
		metrics["training_size"] = len(self.y_train)
		metrics["test_size"] = len(self.y_test)
		metrics["train_time"] = end_train - start_train
		metrics["test_time"] = end_test - start_test
		metrics["fold_id"] = self.current_fold
		metrics["test_idxs"] = self.test_idxs

		if not self.metrics_headers:
			#Create files and Write file headers
			os.mkdir(self.folder_name)
			self.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())
			self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
			self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
			self.metrics_writer.writeheader()

		#Write up metric results
		self.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))

		#Search type changes
		print("acc:", metrics["accuracy"])

		#make space in memory
		del model
		gc.collect()

		self.accuracy_scores.append(metrics["accuracy"])
		if self.current_fold == self.folds:
			average_acc = sum(self.accuracy_scores) / len(self.accuracy_scores)
			print("average acc:", average_acc)

		if self.thresholding:
			if metrics["accuracy"] < self.temp_min_threshold:
				return True
			else:
				#On last fold check if average accuracy > current threshold, update temporary minimum to smallest from folds accuracy
				if average_acc > self.min_threshold:
					self.temp_min_threshold = min(self.accuracy_scores)
					self.min_threshold = average_acc
					print("* * * NEW RECORD avg acc:", average_acc, "min acc:", self.temp_min_threshold)

		return False # Only return true to stop models running


	def run_experiments(self, num_experiments=100):		
		# GRID SEARCH 
		#Find total possible configurations from options
		self.total = 1 
		for key in self.original_h_params:
			self.total *= len(self.original_h_params[key])

		if self.search_algorithm == "grid":
			header_list = list(self.h_params.keys()) #Fixed keys list to loop in order
			countdown = len(self.h_params) - 1
			self.num_experiments = self.total
			print("grid search of ", self.total, "configurations...")
			self.loop_values(header_list, countdown)

		# RANDOM SEARCH 
		elif self.search_algorithm == "random":
			self.num_experiments = num_experiments
			print("random search of ", self.num_experiments, "configurations of a possible", self.total, "configurations")
			while(self.experiment_id <= self.num_experiments):
				self.run_one_experiment()


		# Experiments run - close data files
		print(self.experiment_id, " models run.")

		self.metrics_file.close()


	def loop_values(self, header_list, countdown):
		# loop through all possible configurations in original parameter dictionary
		# http://stackoverflow.com/questions/7186518/function-with-varying-number-of-for-loops-python
		if (countdown > 0):
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.loop_values(header_list, countdown - 1)
		else:	
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.run_one_experiment()



class Increase_Snaphot_Experiment(Experiment):
	"""Experiment to look at change in data snapshot intervals"""
	def __init__(self, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		data=None, folds=10, 
		folder_name=str(time.time()),
		thresholding=False, threshold=0.5, run_on_factors=True):
		
		super(Increase_Snaphot_Experiment, self).__init__(parameters, search_algorithm="grid", 
		x_test=x_test, y_test=y_test,
		x_train=x_train, y_train=y_train, 
		data=data, folds=folds, 
		folder_name=folder_name,
		thresholding=thresholding, threshold=threshold)
		
		self.run_on_factors = run_on_factors

	def set_up_model(self):
		# Keep only every step-th data snapshot, do not run unless new data involved
		if (self.run_on_factors and ((self.current_params["sequence_length"] - 1) % self.current_params["step"] == 0)) or (self.run_on_factors == False): 
			self.x_train = self.x_train[:,::self.current_params["step"]]
			self.x_test = self.x_test[:,::self.current_params["step"]]

			# Shuffle data 
			self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
			self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
			
			# scale data by test data mean and variance
			means, stdvs = get_mean_and_stdv(self.x_train)
			self.x_train = scale_array(self.x_train, means, stdvs)
			self.x_test = scale_array(self.x_test, means, stdvs)

			# Output size - in future delete any cols for categorical which are all zero 
			model = self.generate_model(self.x_train, self.y_train, self.current_params)

			if self.current_fold == 1:
				print(model.summary())
			
			#UNINDENT line BELOW
			return self.train_model(model) #Returns TRUE if accuracy below threshold 


class Ensemble(Experiment):
	def write_up_models(self, models, preds):
		self.metrics["truth"] = self.y_test.flatten()
		self.metrics["preds"] = np.array(preds).mean(axis=0)
		self.metrics["categorical_preds"] = [np.round(x) for x in self.metrics["preds"]]
		self.metrics["fscore"] = f1_score(self.metrics["truth"], self.metrics["categorical_preds"])
		self.metrics["accuracy"] = accuracy_score(self.metrics["truth"], self.metrics["categorical_preds"])

		writeable = merge_two_dicts(self.current_params, self.metrics)

		if not self.metrics_headers:
			#Create files and Write file headers
			os.mkdir(self.folder_name)
			self.metrics_headers = writeable.keys()
			self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
			self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
			self.metrics_writer.writeheader()

		#Search type changes
		print("acc:", self.metrics["accuracy"])

		#make space in memory
		for model in models:
			del model
		gc.collect()


class Ensemble_configurations(Ensemble):
	"""Average the predictions of multiple models passed as a list as configurations"""
	def __init__(self, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		data=None, folds=10, 
		folder_name=str(time.time()),
		thresholding=False, threshold=0.5, batch_size=None):

		super(Ensemble_configurations, self).__init__(parameters[0], search_algorithm="grid", x_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train, data=data, folds=folds, folder_name=folder_name, thresholding=thresholding, threshold=threshold)
		self.search_algorithm = None

		# Sequence length is the only shared variable - get distinct values
		self.sequence_lengths = set([s for seq_lens in [config["sequence_length"] for config in parameters] for s in seq_lens])

		#Only one parameter considered from each config, not a search experiment
		if not(all([all([len(config[key]) == 1 for key in config]) for config in parameters])):
			"not a search experiment, only one parameter will be used from each configuration"

		# dictionary values to list
		self.configurations = []
		for config in parameters:
			config = dict([(key, list(config[key])[0]) for key in config])
			self.configurations.append(config)
			#Can set all batch sizes the same if specified
			if batch_size:
				config["batch_size"] = batch_size


	def set_up_model(self):
		# Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
		self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
		
		# scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)

		# Output size - in future delete any cols for categorical which are all zero 
		models = [self.generate_model(self.x_train, self.y_train, config) for config in self.configurations]

		reset_states = ResetStatesCallback()
		preds = []

		for model, config in zip(models, self.configurations):
			h = model.fit(
				self.x_train, self.y_train, 
				batch_size=config["batch_size"], 
				epochs=config["epochs"],
				verbose=0, 
				shuffle=True,	
				callbacks=[reset_states])

			d = config["description"]

			self.metrics["train_acc_{}".format(d)] = h.history["acc".format(d)]
			pred_Y = model.predict(self.x_test, batch_size=config["batch_size"])
			preds.append(pred_Y)
			self.metrics["preds_{}".format(d)] = [x[0] for x in pred_Y]
			self.metrics["acc_{}".format(d)] = accuracy_score(self.y_test.flatten(), [np.round(x) for x in self.metrics["preds_{}".format(d)]])
			print(d, "acc :", self.metrics["acc_{}".format(d)])

		self.write_up_models(models, preds)
		return False
	
	def run_experiments(self):		
		# Only changeable parameter is sequence length
		self.num_experiments = len(self.sequence_lengths)

		for s in self.sequence_lengths:
			self.current_params = {"sequence_length": s}
			self.run_one_experiment()

		# Experiments run - close data files
		print(self.experiment_id, " models run.")

		self.metrics_file.close()


class Ensemble_sub_sequences(Ensemble):
	"""Ensemble models for sub-seqeunces of data"""
	def __init__(self, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		data=None, folds=10, 
		folder_name=str(time.time()),
		thresholding=False, threshold=0.5):

		super(Ensemble_sub_sequences, self).__init__(parameters, search_algorithm="grid", x_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train, data=data, folds=folds, folder_name=folder_name, thresholding=thresholding, threshold=threshold)
		self.search_algorithm = None

		# Sequence length is the only shared variable - get distinct values
		self.sequence_lengths = self.h_params["sequence_length"]

	def set_up_model(self):
		# Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
		self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
		
		# scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)

		# Output size - in future delete any cols for categorical which are all zero 
		models = []
		reset_states = ResetStatesCallback()
		metrics = self.metrics
		preds = []

		training_sets = []
		testing_sets = []
		idx_len_tuples = []

		#Create multiple training sets
		for length in list(range(1, self.current_params["sequence_length"])):
			mini_train_X, b = into_sliding_chunk_arrays(self.x_train, length)
			mini_test_X, b = into_sliding_chunk_arrays(self.x_test, length)
			idx_len_tuples += b
			training_sets += mini_train_X #append for merge2
			testing_sets += mini_test_X

		#Finally add whole sets
		training_sets.append(self.x_train)
		testing_sets.append(self.x_test)
		idx_len_tuples.append((0, self.current_params["sequence_length"]))

		for i, train_set in enumerate(training_sets):
			d = idx_len_tuples[i]
			self.x_train = train_set
			self.x_test =  testing_sets[i]

			#Output size - in future delete any cols for categorical which are all zero 
			model = self.generate_model(self.x_train, self.y_train, self.current_params)

			h = model.fit(self.x_train, self.y_train, batch_size=self.current_params["batch_size"], epochs=self.current_params["epochs"], verbose=0, shuffle=True, callbacks=[reset_states])

			self.metrics["train_acc_{}".format(d)] = h.history["acc"]
			pred_Y = model.predict(self.x_test, batch_size=self.current_params["batch_size"])
			preds.append(pred_Y)
			self.metrics["preds_{}".format(d)] = [x[0] for x in pred_Y]
			self.metrics["acc_{}".format(d)] = accuracy_score(self.y_test.flatten(), [np.round(x) for x in self.metrics["preds_{}".format(d)]])
			print(d, "acc :", self.metrics["acc_{}".format(d)])

		self.write_up_models(models, preds)
		return False

	def run_experiments(self):		
		# Only changeable parameter is sequence length
		self.num_experiments = len(self.sequence_lengths)

		for s in self.sequence_lengths:
			self.run_one_experiment()

		# Experiments run - close data files
		print(self.experiment_id, " models run.")

		self.metrics_file.close()


class Omit_test_data(Experiment):
	def train_model(self, model):
		"""run one fold and write up results"""
		print("		fold ", self.current_fold, "of", self.folds)
		metrics = self.metrics
		reset_states = ResetStatesCallback()

		model.fit(
		self.x_train, self.y_train, 
		batch_size=self.current_params["batch_size"], 
		epochs=self.current_params["epochs"],
		verbose=0, 
		shuffle=True,	
		callbacks=[reset_states])

		headers = [x + "_ON" for x in self.headers[:len(self.x_test[0][0])]]

		indicies = list(range(len(headers)))

		for num_missing in range(1, len(indicies)+1):
			for subset in combinations(indicies, num_missing):
				temp_test_X = deepcopy(self.x_test)
				states = [1 for x in range(len(indicies))]

				for feature_index in subset:
					states[feature_index] = 0
					temp_test_X[:,:,feature_index] = 0 # because 0 is the mean of the training data
			
				pred_Y = model.predict(temp_test_X, batch_size=self.current_params["batch_size"]) #Predicted value of Y
				pred_Y = [x[0] for x in pred_Y]
				Y_classes =  [np.round(x) for x in pred_Y] #Choose a class
				metrics["preds"] = pred_Y
				metrics["categorical_preds"] = Y_classes
				metrics["truths"] = [int(x[0]) for x in self.y_test]
				metrics["variance"] = np.var(pred_Y)
				metrics["accuracy"] = accuracy_score(self.metrics["truths"], self.metrics["categorical_preds"])
				metrics["fmeasure"] = accuracy_score(self.metrics["truths"], self.metrics["categorical_preds"])



				if not self.metrics_headers:
					#Create files and Write file headers
					os.mkdir(self.folder_name)
					self.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())
					self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
					self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
					self.metrics_writer.writeheader()

				#Write up metric results
				self.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))

				#Search type changes
				print("acc:", metrics["accuracy"], "features on:", str(states))

		#make space in memory
		del model
		gc.collect()



