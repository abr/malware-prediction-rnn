from keras import callbacks
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU, Bidirectional
from keras.regularizers import l1, l2, l1_l2
from sklearn.metrics import f1_score, accuracy_score
import numpy as np
import random
import operator
from copy import deepcopy
import csv
from useful.EditCSV import *
import time
import os
from keras.constraints import *
from collections import Counter
import gc

class Experiment():
	"""docstring for Experiment"""
	def __init__(self, model_generator, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		x_val=None, y_val=None,
		data=None, folds=None, 
		folder_name="Folder name",
		thresholding=False, thrshold=0.5):
		
		assert type(parameters) == dict

		#hyperparameters
		self.original_h_params = parameters
		self.h_params = parameters

		# deal with any list instances
		self.__list_to_dict_params()
		self.current_params = None
		
		#search algorithm
		self.search_algorithm = search_algorithm
		self.thresholding = thresholding
		if self.search_algorithm == "grid":
			self.current_params = {}
			for key in self.h_params:
				possibles = sorted(list(self.h_params[key]))
				self.h_params[key] = possibles
				self.original_h_params[key] = possibles
				print(possibles)
				self.current_params[key] = possibles[0]

			print(self.current_params)				
		if self.search_algorithm == "random":
			self.get_config = self.__random_config
			self.__map_to_0_1()

		#print(self.h_params)

		#metrics and write-up
		self.folder_name = check_filename(folder_name)
		self.experiment_id = 0
		self.metrics_headers = None
		self.metrics_writer = None 
		self.distribution_writer = None

		#Model generator
		self.__generate_model = model_generator
		
		#experiment set up 
		self.folds = folds
		if self.thresholding:
			self.min_threshold = self.threshold + K.epsilon()
			self.temp_min_threshold = self.threshold + K.epsilon()

		#If test set supplied
		if self.folds == None:
			self.X_TRAIN = x_train
			self.Y_TRAIN = y_train
			self.X_TEST = x_test
			self.Y_TEST = y_test
		else: #x-fold CV
			self.x = data[0]
			self.y = data[1]


		#If validation data supplied
		if (x_val is not None) and (y_val is not None):
			self.val_data = True
			self.X_VAL = x_val
			self.Y_VAL = y_val
		else:
			self.val_data = False 

	def __list_to_dict_params(self):
			for key in self.h_params:
				if type(self.h_params[key]) is list:
					self.h_params[key] = dict([(x, 1/(len(self.h_params[key]) + K.epsilon() )) for x in self.h_params[key]])

	def __map_to_0_1(self):
		"""maps input probabilities to values between 0 and 1, preserving scalar relationships"""
		for key in self.h_params:
			running_total = 0
			scalar = 1/(sum(self.h_params[key].values()) + K.epsilon())
			for possible_value in self.h_params[key]:
				if self.h_params[key][possible_value] < 0:
					raise ValueError("Negative hyperparameter probabilities are not allowed ({} for {})").format(self.h_params[key][possible_value], possible_value)
				new_value = self.h_params[key][possible_value]*scalar
				self.h_params[key][possible_value] = new_value + running_total
				running_total += new_value


	def __random_config(self):
		"""randomly generate a configuration of hyperparameters from dictionary self.h_params"""
		temp_params = deepcopy(self.h_params)
		for key in self.h_params:
			choice = random.random()
			sorted_options = sorted(self.h_params[key].items(), key=operator.itemgetter(1))
			for option in sorted_options:
				if choice < option[1]:
					temp_params[key] = option[0]
					break
				elif option[1] > 0.9999999:
					temp_params[key] = option[0] #in case of rounding error in __map_to_0_1()
					break

		self.current_params = temp_params
		for i in self.current_params:
			print(i, ":", self.current_params[i], end=" - ")
		print()


	def __reshape_x_dims(self, x, y):
		x_shape = [len(x)]
		y_shape = [len(y), 1]
		check_for_dims = True
		array = x
		while check_for_dims:
			try:
				array = array[0]
				x_shape.append(len(array))
			except TypeError:
				check_for_dims = False
				x = x.reshape(tuple(x_shape))
				y = y.reshape(tuple(y_shape))

		return x, y


	def run_one_experiment(self):

		#Get new configuration if random search
		if self.search_algorithm == "random":
			self.get_config()
		
		total_length = self.original_h_params["sequence_length"][0]

		for length in list(range(self.current_params["step"]*2, total_length, self.current_params["step"]))[::-1]:
			self.current_params["sequence_length"] = length

			self.experiment_id += 1
			print("running expt", self.experiment_id, "of", self.num_experiments)
			
			self.metrics = {}
			self.current_fold = 1
			self.averages = []

			print("step", self.current_params["step"], "this length", self.current_params["sequence_length"], "total length", total_length)
			
			if self.folds != None:
				#Split up data into folds
				y = deepcopy(self.y)
				x = deepcopy(self.x)

				x, y = remove_short(x, y, self.current_params["sequence_length"]) # remove short sequences BEFORE changing step interval
				x, y = self.__reshape_x_dims(x, y)
				x = x[:,::self.current_params["step"]] # change interval between snapshots

				print(x.shape, y.shape)

				labels = {}
				temp_y = deepcopy(y).flatten() # get labels as flat array to find stratified folds

				for i, class_label in enumerate(temp_y):
					if class_label in labels:
						labels[class_label].append(i)
					else:
						labels[class_label] = [i]

				fold_indicies = [[] for x in range(self.folds)]

				# Want to represent class distribution in each set (stratified split)
				# Divide indicies list in chunks of size folds
				for key in labels:
					labels[key] = to_chunks(labels[key], self.folds)
					for i, fold_ids in enumerate(labels[key]):
						fold_indicies[i] += fold_ids

				fold_indicies = np.array([[int(x) for x in index_set] for index_set in fold_indicies])
				
				#Set test, train, val sets
				
				for i in list(range(self.folds)):
					#val = np.array([i % self.folds])
					test = np.array([(i) % self.folds]) # one fold is test set
					train = np.array([i for i in range(self.folds) if i not in [test]]) # remaining folds are training set ( not in [test, val])

					test_idxs = np.concatenate(tuple([fold_indicies[x] for x in test]))
					train_idxs = np.concatenate(tuple([fold_indicies[x] for x in train]))

					self.x_train = x[train_idxs]
					self.y_train = y[train_idxs]
					self.x_test = x[test_idxs]
					self.y_test = y[test_idxs]

					if self.val_data:
						self.x_val, self.y_val = remove_short(self.x_val, self.y_val, self.current_params["sequence_length"])
						self.x_val, self.y_val = self.__reshape_x_dims(self.x_val, self.y_val)

					stop = self.__train_and_test_model()
					if stop:
						return

					self.current_fold += 1

			
			else:
				#Split up data into folds
				self.x_train = deepcopy(self.X_TRAIN)
				self.y_train = deepcopy(self.Y_TRAIN)
				self.x_test = deepcopy(self.X_TEST)
				self.y_test = deepcopy(self.Y_TEST)

				#remove short if recurrent 
				if self.current_params["layer_type"] in ["GRU", "LSTM"]:
					self.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params["sequence_length"])
					self.x_test, self.y_test = remove_short(self.x_test, self.y_test, self.current_params["sequence_length"])
					

				#into x-dimensional tensors:
				self.x_test, self.y_test = self.__reshape_x_dims(self.x_test, self.y_test)
				self.x_train, self.y_train = self.__reshape_x_dims(self.x_train, self.y_train)
			
				if self.val_data:
					self.x_val = deepcopy(self.X_VAL)
					self.y_val = deepcopy(self.Y_VAL)
					if self.current_params["layer_type"] in ["GRU", "LSTM"]:
						self.x_val, self.y_val = remove_short(self.x_val, self.y_val, self.current_params["sequence_length"])
					self.x_val, self.y_val = self.__reshape_x_dims(self.x_val, self.y_val)


				self.__train_and_test_model()
		

	def __train_and_test_model(self):
		# FOR both test-train and 10-fold
		print("test, train set size (x):", self.x_test.shape, self.x_train.shape,)
		print("test, train set size (y):", self.y_test.shape, self.y_train.shape,)
		#Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies(self.x_train, self.y_train)
		self.x_test, self.y_test = unison_shuffled_copies(self.x_test, self.y_test)
		
		#scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)
		if self.val_data:
			self.x_val = scale_array(self.x_val, means, stdvs)

		#Output size - in future delete any cols for categorical which are all zero 
		model = self.__generate_model(self.x_train, self.y_train, self.current_params)

		if self.current_fold == 1:
			print(model.summary())

		return self.__train_model(model) #Returns TRUE if accuracy below threshold 
			

	def __train_model(self, model):
		"""run one fold and write up results"""
		print("		fold ", self.current_fold, "of", self.folds)
		metrics = self.metrics
		reset_states = ResetStatesCallback()

		start_train = time.time()
		if self.val_data:
			h = model.fit(
			self.x_train, self.y_train, 
			batch_size=self.current_params["batch_size"], 
			epochs=self.current_params["epochs"],
			verbose=0, 
			shuffle=True, 
			validation_data=(self.x_val, self.y_val),
			callbacks=[reset_states])
		else:
			h = model.fit(
				self.x_train, self.y_train, 
				batch_size=self.current_params["batch_size"], 
				epochs=self.current_params["epochs"],
				verbose=0, 
				shuffle=True,	
				callbacks=[reset_states])

		end_train = time.time()

		if self.val_data:
			metrics["val_acc"] = h.history["val_acc"]

		metrics["train_acc"] = h.history["acc"]

		start_test = time.time()
		__pred_Y = model.predict(self.x_test, batch_size=self.current_params["batch_size"])
		metrics["preds"] = [x[0] for x in __pred_Y]
		end_test = time.time()
		metrics["truth"] = self.y_test.flatten()

		if self.current_params["loss"] == "categorical_crossentropy":
			metrics["truth"] = [list(x).index(max(x)) for x in metrics["truth"]]
			categorical_preds = [list(x).index(max(x)) for x in metrics["preds"]]
		else:
			#if "nan" in metrics["preds"][0][0]:
			#	return
			metrics["categorical_preds"] = [np.round(x) for x in metrics["preds"]]
			try:
				metrics["fscore"] = f1_score(metrics["truth"], metrics["categorical_preds"])
				metrics["accuracy"] = accuracy_score(metrics["truth"], metrics["categorical_preds"])
			except ValueError:
				metrics["fscore"] = 0.667
				metrics["accuracy"] = 0.5
				
		metrics["experiment_id"] = self.experiment_id
		metrics["training_size"] = len(self.y_train)
		metrics["test_size"] = len(self.y_test)
		metrics["train_time"] = end_train - start_train
		metrics["test_time"] = end_test - start_test
		metrics["fold_id"] = self.current_fold


		if not self.metrics_headers:
			#Create files and Write file headers
			os.mkdir(self.folder_name)
			self.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())
			self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
			self.distribution_file = open("{}/parameter_distributions.csv".format(self.folder_name), "w")
			self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
			self.distribution_writer = csv.DictWriter(self.distribution_file, fieldnames=list(self.current_params.keys()) + ["experiment_id"])
			self.metrics_writer.writeheader()
			self.distribution_writer.writeheader()

		#Write up metric results and 
		self.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))
		self.distribution_writer.writerow(merge_two_dicts(self.h_params, {"experiment_id": self.experiment_id}))

		#Search type chages
		print("acc:", metrics["accuracy"])

		#make space in memory
		del model
		gc.collect()

		if self.thresholding:
			if metrics["accuracy"] < self.temp_min_threshold:
				return True
			else:
				self.averages.append(metrics["accuracy"])

			#On last fold check if average accuracy > current threshold, update temporary minimum to smallest from folds accuracy
			if self.current_fold == self.folds:
				average_acc = sum(self.averages) / len(self.averages)
				print("average acc:", average_acc)
				if average_acc > self.min_threshold:
					self.temp_min_threshold = min(self.averages)
					self.min_threshold = average_acc
					print("		NEW RECORD av:", average_acc, "min", self.temp_min_threshold)


		return False #Only return true to stop models running. 


	def run_experiments(self, num_experiments=100):		
		#Find total possible configurations from options
		self.total = 1 
		for key in self.original_h_params:
			self.total *= len(self.original_h_params[key])

		# GRID SEARCH 
		if self.search_algorithm == "grid":
			header_list = list(self.h_params.keys()) #Fixed keys list to loop in order
			countdown = len(self.h_params) - 1
			self.num_experiments = self.total
			print("grid search of ", self.total, "configurations...")
			self.loop_values(header_list, countdown)

		# RANDOM SEARCH 
		elif self.search_algorithm == "random":
			self.num_experiments = num_experiments
			print("random search of ", self.num_experiments, "configurations of a possible", self.total, "configurations")
			while(self.experiment_id <= self.num_experiments):
				self.run_one_experiment()
		else: 
			print("ERROR: search algorithm not recognised")

		# Finished running and close files
		print(self.experiment_id, " models run.")

		self.distribution_file.close()
		self.metrics_file.close()


	def loop_values(self, header_list, countdown):
		#http://stackoverflow.com/questions/7186518/function-with-varying-number-of-for-loops-python
		if (countdown > 0):
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.loop_values(header_list, countdown - 1)
		else:	
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.run_one_experiment()


class ResetStatesCallback(callbacks.Callback):
	def __init__(self):
		self.current_epoch = 0

	def on_batch_begin(self, batch, logs={}):
		self.model.reset_states()

	def on_epoch_end(self, epoch, logs={}):
		if epoch > 50:
			if logs.get("acc") <= 0.500001:
				self.model.stop_training = True