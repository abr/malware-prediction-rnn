from keras import callbacks
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU, Bidirectional
from keras.regularizers import l1, l2, l1_l2
from sklearn.metrics import f1_score, accuracy_score
import numpy as np
import random
import operator
from copy import deepcopy
import csv
from useful import *
import time
import os
from keras.constraints import *
from collections import Counter
import gc

class Experiment():
	"""docstring for Experiment"""
	def __init__(self, model_generator, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		x_val=None, y_val=None,
		data=None, folds=None, 
		folder_name="Folder name",
		thresholding=False, thrshold=0.5):
		
		assert type(parameters) == dict

		#hyperparameters
		self.original_h_params = parameters
		self.h_params = parameters

		# deal with any list instances
		self.__list_to_dict_params()
		self.current_params = None
		
		#search algorithm
		self.search_algorithm = search_algorithm
		self.thresholding = thresholding
		if self.search_algorithm == "grid":
			self.current_params = {}
			for key in self.h_params:
				possibles = sorted(list(self.h_params[key]))
				self.h_params[key] = possibles
				self.original_h_params[key] = possibles
				print(possibles)
				self.current_params[key] = possibles[0]

			print(self.current_params)				
		if self.search_algorithm == "random":
			self.get_config = self.__random_config
			self.__map_to_0_1()

		#print(self.h_params)

		#metrics and write-up
		self.folder_name = check_filename(folder_name)
		self.experiment_id = 0
		self.metrics_headers = None
		self.metrics_writer = None 
		self.distribution_writer = None

		#Model generator
		self.__generate_model = model_generator
		
		#experiment set up 
		self.folds = folds
		if self.thresholding:
			self.min_threshold = self.threshold + K.epsilon()
			self.temp_min_threshold = self.threshold + K.epsilon()

		#If test set supplied
		if self.folds == None:
			self.X_TRAIN = x_train
			self.Y_TRAIN = y_train
			self.X_TEST = x_test
			self.Y_TEST = y_test
		else: #x-fold CV
			self.x = data[0]
			self.y = data[1]


		#If validation data supplied
		if (x_val is not None) and (y_val is not None):
			self.val_data = True
			self.X_VAL = x_val
			self.Y_VAL = y_val
		else:
			self.val_data = False 

	def __list_to_dict_params(self):
			for key in self.h_params:
				if type(self.h_params[key]) is list:
					self.h_params[key] = dict([(x, 1/(len(self.h_params[key]) + K.epsilon() )) for x in self.h_params[key]])

	def __map_to_0_1(self):
		"""maps input probabilities to values between 0 and 1, preserving scalar relationships"""
		for key in self.h_params:
			running_total = 0
			scalar = 1/(sum(self.h_params[key].values()) + K.epsilon())
			for possible_value in self.h_params[key]:
				if self.h_params[key][possible_value] < 0:
					raise ValueError("Negative hyperparameter probabilities are not allowed ({} for {})").format(self.h_params[key][possible_value], possible_value)
				new_value = self.h_params[key][possible_value]*scalar
				self.h_params[key][possible_value] = new_value + running_total
				running_total += new_value


	def __random_config(self):
		"""randomly generate a configuration of hyperparameters from dictionary self.h_params"""
		temp_params = deepcopy(self.h_params)
		for key in self.h_params:
			choice = random.random()
			sorted_options = sorted(self.h_params[key].items(), key=operator.itemgetter(1))
			for option in sorted_options:
				if choice < option[1]:
					temp_params[key] = option[0]
					break
				elif option[1] > 0.9999999:
					temp_params[key] = option[0] #in case of rounding error in __map_to_0_1()
					break

		self.current_params = temp_params
		for i in self.current_params:
			print(i, ":", self.current_params[i], end=" - ")
		print()


	def __reshape_x_dims(self, x, y):
		x_shape = [len(x)]
		y_shape = [len(y), 1]
		check_for_dims = True
		array = x
		while check_for_dims:
			try:
				array = array[0]
				x_shape.append(len(array))
			except TypeError:
				check_for_dims = False
				x = x.reshape(tuple(x_shape))
				y = y.reshape(tuple(y_shape))

		return x, y


	def run_one_experiment(self):

		#Get new configuration if random search
		if self.search_algorithm == "random":
			self.get_config()
		
		self.experiment_id += 1
		print("running expt", self.experiment_id, "of", self.num_experiments)
		
		self.metrics = {}
		self.current_fold = 1
		self.averages = []

		if self.folds != None:
			#Split up data into folds
			y = deepcopy(self.y)
			x = deepcopy(self.x)

			print(1, x.shape, type(x), y.shape, type(y))

			#remove short and store indicies of kept items
			if self.current_params["layer_type"] in ["GRU", "LSTM"]:
				x, y, identifiers = remove_short_idx(x, y, list(range(len(y))), self.current_params["sequence_length"])

			labels = {}
			temp_y = deepcopy(y).flatten() # get labels as flat array to find stratified folds

			print(2, x.shape, type(x), y.shape, type(y))

			for i, class_label in zip(identifiers, temp_y):
				if class_label in labels:
					labels[class_label].append(i)
				else:
					labels[class_label] = [i]

			#split into # of folds
			fold_indicies = [[] for x in range(self.folds)]

			# Want to represent class distribution in each set (stratified split)
			# Divide indicies list in chunks of size folds
			for key in labels:
				labels[key] = to_chunks(labels[key], self.folds)
				for i, fold_ids in enumerate(labels[key]):
					fold_indicies[i] += fold_ids

			fold_indicies = np.array([[int(x) for x in index_set] for index_set in fold_indicies])

			print(3, x.shape, type(x), y.shape, type(y))

			#take new copies of the original to use indicies from the original sets
			x, y = deepcopy(self.x), deepcopy(self.y)
			print(4, x.shape, type(x), y.shape, type(y))

			#Set test, train, val sets
			if self.val_data:
				self.x_val, self.y_val = remove_short(self.x_val, self.y_val, self.current_params["sequence_length"])
				self.x_val, self.y_val = self.__reshape_x_dims(self.x_val, self.y_val)

			for i in list(range(self.folds)):
				#val = np.array([i % self.folds])
				test = np.array([(i) % self.folds]) # one fold is test set
				train = np.array([i for i in range(self.folds) if i not in [test]]) # remaining folds are training set ( not in [test, val])

				test_idxs = np.concatenate(tuple([fold_indicies[i] for i in test]))
				train_idxs = np.concatenate(tuple([fold_indicies[i] for i in train]))

				self.x_train = x[train_idxs]
				self.y_train = y[train_idxs]
				self.x_test = x[test_idxs]
				self.y_test = y[test_idxs]

				self.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params["sequence_length"])
				self.x_test, self.y_test = remove_short(self.x_test, self.y_test, self.current_params["sequence_length"])

				print(self.x_train.shape, type(self.x_train))

				# reshape into tensors for model 
				self.x_train, self.y_train = self.__reshape_x_dims(self.x_train, self.y_train)
				self.x_test, self.y_train = self.__reshape_x_dims(self.x_test, self.y_train)

				self.test_idxs = test_idxs

				stop = self.__train_and_test_model()
				if stop:
					return

				self.current_fold += 1

		
		else:
			#Split up data into folds
			self.x_train = deepcopy(self.X_TRAIN)
			self.y_train = deepcopy(self.Y_TRAIN)
			self.x_test = deepcopy(self.X_TEST)
			self.y_test = deepcopy(self.Y_TEST)

			self.test_idxs = np.array(range(1, len(self.y_test) + 1)) / 10 #divide by 10 to distinguish from training/10-fold data

			#remove short if recurrent 
			if self.current_params["layer_type"] in ["GRU", "LSTM"]:
				self.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params["sequence_length"])
				self.x_test, self.y_test, self.test_idxs = remove_short_idx(self.x_test, self.y_test, self.test_idxs, self.current_params["sequence_length"])
			

			#into x-dimensional tensors:
			self.x_test, self.y_test = self.__reshape_x_dims(self.x_test, self.y_test)
			self.x_train, self.y_train = self.__reshape_x_dims(self.x_train, self.y_train)
		
			if self.val_data:
				self.x_val = deepcopy(self.X_VAL)
				self.y_val = deepcopy(self.Y_VAL)
				if self.current_params["layer_type"] in ["GRU", "LSTM"]:
					self.x_val, self.y_val = remove_short(self.x_val, self.y_val, self.current_params["sequence_length"])
				
				self.x_val, self.y_val = self.__reshape_x_dims(self.x_val, self.y_val)


			self.__train_and_test_model()
		

	def __train_and_test_model(self):
		# FOR both test-train and 10-fold
		print("test, train set size (x):", self.x_test.shape, self.x_train.shape,)
		print("test, train set size (y):", self.y_test.shape, self.y_train.shape,)
		#Shuffle data 
		train_X, self.y_train = unison_shuffled_copies_list([self.x_train, self.y_train])
		test_X, self.y_test, self.test_idxs = unison_shuffled_copies_list([self.x_test, self.y_test, self.test_idxs])
		

		# EXTRA STUFF FOR THIS MODEL
		training_sets = []
		testing_sets = []
		idx_len_tuples = []

		#Create multiple training sets
		for length in list(range(2, self.current_params["sequence_length"])):
			mini_train_X, b = into_sliding_chunk_arrays(train_X, length)
			mini_test_X, b = into_sliding_chunk_arrays(test_X, length)
			idx_len_tuples += b
			training_sets += mini_train_X #append for merge2
			testing_sets += mini_test_X

		#Finally add whole sets
		training_sets.append(train_X)
		testing_sets.append(test_X)
		idx_len_tuples.append((0, self.current_params["sequence_length"]))

		for i, train_set in enumerate(training_sets):
			print(idx_len_tuples[i], "folds", self.current_fold)
			self.x_train = train_set
			self.x_test =  testing_sets[i]

			#scale data by test data mean and variance
			means, stdvs = get_mean_and_stdv(self.x_train)
			self.x_train = scale_array(self.x_train, means, stdvs)
			self.x_test = scale_array(self.x_test, means, stdvs)
			if self.val_data:
				self.x_val = scale_array(self.x_val, means, stdvs)

			#Output size - in future delete any cols for categorical which are all zero 
			model = self.__generate_model(self.x_train, self.y_train, self.current_params)

			return self.__train_model(model) #Returns TRUE if accuracy below threshold 
		

	def __train_model(self, model):
		"""run one fold and write up results"""
		print("		fold ", self.current_fold, "of", self.folds)
		metrics = self.metrics
		reset_states = ResetStatesCallback()

		start_train = time.time()
		if self.val_data:
			h = model.fit(
			self.x_train, self.y_train, 
			batch_size=self.current_params["batch_size"], 
			epochs=self.current_params["epochs"],
			verbose=0, 
			shuffle=True, 
			validation_data=(self.x_val, self.y_val),
			callbacks=[reset_states])
		else:
			h = model.fit(
				self.x_train, self.y_train, 
				batch_size=self.current_params["batch_size"], 
				epochs=self.current_params["epochs"],
				verbose=0, 
				shuffle=True,	
				callbacks=[reset_states])

		end_train = time.time()

		if self.val_data:
			metrics["val_acc"] = h.history["val_acc"]

		metrics["train_acc"] = h.history["acc"]

		start_test = time.time()
		__pred_Y = model.predict(self.x_test, batch_size=self.current_params["batch_size"])
		metrics["preds"] = [x[0] for x in __pred_Y]
		end_test = time.time()
		metrics["truth"] = self.y_test.flatten()

		if self.current_params["loss"] == "categorical_crossentropy":
			metrics["truth"] = [list(x).index(max(x)) for x in metrics["truth"]]
			categorical_preds = [list(x).index(max(x)) for x in metrics["preds"]]
		else:
			#if "nan" in metrics["preds"][0][0]:
			#	return
			metrics["categorical_preds"] = [np.round(x) for x in metrics["preds"]]
			try:
				metrics["fscore"] = f1_score(metrics["truth"], metrics["categorical_preds"])
				metrics["accuracy"] = accuracy_score(metrics["truth"], metrics["categorical_preds"])
			except ValueError:
				metrics["fscore"] = 0.667
				metrics["accuracy"] = 0.5
				
		metrics["experiment_id"] = self.experiment_id
		metrics["training_size"] = len(self.y_train)
		metrics["test_size"] = len(self.y_test)
		metrics["train_time"] = end_train - start_train
		metrics["test_time"] = end_test - start_test
		metrics["fold_id"] = self.current_fold
		assert len(self.test_idxs) == len(metrics["preds"])
		metrics["test_idxs"] = self.test_idxs


		if not self.metrics_headers:
			#Create files and Write file headers
			os.mkdir(self.folder_name)
			self.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())
			self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
			self.distribution_file = open("{}/parameter_distributions.csv".format(self.folder_name), "w")
			self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
			self.distribution_writer = csv.DictWriter(self.distribution_file, fieldnames=list(self.current_params.keys()) + ["experiment_id"])
			self.metrics_writer.writeheader()
			self.distribution_writer.writeheader()

		#Write up metric results and 
		self.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))
		self.distribution_writer.writerow(merge_two_dicts(self.h_params, {"experiment_id": self.experiment_id}))

		#Search type chages
		print("acc:", metrics["accuracy"])

		#make space in memory
		del model
		gc.collect()

		if self.thresholding:
			if metrics["accuracy"] < self.temp_min_threshold:
				return True
			else:
				self.averages.append(metrics["accuracy"])

			#On last fold check if average accuracy > current threshold, update temporary minimum to smallest from folds accuracy
			if self.current_fold == self.folds:
				average_acc = sum(self.averages) / len(self.averages)
				print("average acc:", average_acc)
				if average_acc > self.min_threshold:
					self.temp_min_threshold = min(self.averages)
					self.min_threshold = average_acc
					print("		NEW RECORD av:", average_acc, "min", self.temp_min_threshold)


		return False #Only return true to stop models running. 


	def run_experiments(self, num_experiments=100):		
		#Find total possible configurations from options
		self.total = 1 
		for key in self.original_h_params:
			self.total *= len(self.original_h_params[key])

		# GRID SEARCH 
		if self.search_algorithm == "grid":
			header_list = list(self.h_params.keys()) #Fixed keys list to loop in order
			countdown = len(self.h_params) - 1
			self.num_experiments = self.total
			print("grid search of ", self.total, "configurations...")
			self.loop_values(header_list, countdown)

		# RANDOM SEARCH 
		elif self.search_algorithm == "random":
			self.num_experiments = num_experiments
			print("random search of ", self.num_experiments, "configurations of a possible", self.total, "configurations")
			while(self.experiment_id <= self.num_experiments):
				self.run_one_experiment()
		else: 
			print("ERROR: search algorithm not recognised")

		# Finished running and close files
		print(self.experiment_id, " models run.")

		self.distribution_file.close()
		self.metrics_file.close()


	def loop_values(self, header_list, countdown):
		#http://stackoverflow.com/questions/7186518/function-with-varying-number-of-for-loops-python
		if (countdown > 0):
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.loop_values(header_list, countdown - 1)
		else:	
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.run_one_experiment()


class ResetStatesCallback(callbacks.Callback):
	def __init__(self):
		self.current_epoch = 0

	def on_batch_begin(self, batch, logs={}):
		self.model.reset_states()

	def on_epoch_end(self, epoch, logs={}):
		if epoch > 50:
			if logs.get("acc") <= 0.500001:
				self.model.stop_training = True