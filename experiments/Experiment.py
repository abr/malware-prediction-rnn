import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU, Bidirectional
from keras.regularizers import l1, l2, l1_l2
from sklearn.metrics import f1_score, accuracy_score
import numpy as np
import random
import operator
from copy import deepcopy
import csv
from useful import *
from RNN import generate_model
import time
import os
from collections import Counter
import gc

class Experiment():
	"""docstring for Experiment"""
	def __init__(self, parameters, search_algorithm="grid", 
		x_test=None, y_test=None,
		x_train=None, y_train=None, 
		data=None, folds=10, 
		folder_name=str(time.time()),
		thresholding=False, threshold=0.5):
		
		assert type(parameters) == dict, "parameters must be provided as a dictionary"
		assert (data == None) or (x_test == y_test == x_train == y_train == None), "provide EITHER data tuple for k-fold cross validation OR x_test, y_test, x_train, y_train"
		assert (type(folds) is int) or (data == None), "folds must be an integer if data tuple is provided"
		assert (search_algorithm.lower() == "grid") or (search_algorithm.lower() == "random"), "Only 'grid' and 'random' permissible values for search_algorithm"

		#hyperparameters
		self.original_h_params = parameters
		self.h_params = parameters		
		
		# set up parameter search space depending on algorithm
		self.search_algorithm = search_algorithm
		self.current_params = None

		if self.search_algorithm == "grid":
			self.current_params = {}
			self.h_params = dict([(key, sorted(list(self.h_params[key]))) for key in self.h_params])
			self.original_h_params = deepcopy(self.h_params)
			self.current_params = dict([(key, self.h_params[key][0]) for key in self.h_params])
		if self.search_algorithm == "random":
			self.__list_to_dict_params() # list instances to even probability dictionary instances
			self.get_config = self.__random_config()
			self.__map_to_0_1() # re-adjust any instances in which sum(probabilities) > 1

		# metrics and writer objects, to be assigned when first experiment written up
		self.folder_name = check_filename(folder_name)
		self.experiment_id = 0
		self.metrics_headers = None
		self.metrics_writer = None 

		#Model generator
		self.__generate_model = generate_model
		
		# Thresholding set up
		self.thresholding = thresholding
		if self.thresholding:
			self.min_threshold = threshold + K.epsilon()
			self.temp_min_threshold = threshold + K.epsilon()

		# test-train experiment
		if self.folds == None:
			self.X_TRAIN = x_train
			self.Y_TRAIN = y_train
			self.X_TEST = x_test
			self.Y_TEST = y_test
		# k-fold cross-validation experiment
		else: 
			self.x = data[0]
			self.y = data[1]


	def __list_to_dict_params(self):
			for key in self.h_params:
				if type(self.h_params[key]) is list:
					self.h_params[key] = dict([(x, 1/(len(self.h_params[key]) + K.epsilon() )) for x in self.h_params[key]])

	def __map_to_0_1(self):
		"""maps input probabilities to values between 0 and 1, preserving scalar relationships"""
		for key in self.h_params:
			running_total = 0
			scalar = 1/(sum(self.h_params[key].values()))
			for possible_value in self.h_params[key]:
				if self.h_params[key][possible_value] < 0:
					raise ValueError("Negative hyperparameter probabilities are not allowed ({} for {})").format(self.h_params[key][possible_value], possible_value)
				new_value = self.h_params[key][possible_value] * scalar
				self.h_params[key][possible_value] = new_value + running_total
				running_total += new_value

	def __random_config(self):
		"""randomly generate a configuration of hyperparameters from dictionary self.h_params"""
		for key in self.h_params:
			choice = random.random()
			sorted_options = sorted(self.h_params[key].items(), key=operator.itemgetter(1))
			for option in sorted_options:
				if choice < option[1]:
					self.current_params[key] = option[0]
					print(key, ":", self.current_params[key], end=" - ")
					break
			print()

	def run_one_experiment(self):
		#Get new configuration if random search
		if self.search_algorithm == "random":
			self.get_config()
		
		self.experiment_id += 1
		print("running expt", self.experiment_id, "of", self.num_experiments)
		
		self.metrics = {}
		self.current_fold = 1
		self.accuracy_scores = []

		# k-fold cross-validation
		if self.folds != None:
			y = deepcopy(self.y)
			x = deepcopy(self.x)

			#remove short seqeunces and store indicies of kept items
			x, y, identifiers = remove_short_idx(x, y, list(range(len(y))), self.current_params["sequence_length"])

			labels = {}
			temp_y = deepcopy(y).flatten() # get labels as flat array to find stratified folds

			for i, class_label in zip(identifiers, temp_y):
				if class_label in labels:
					labels[class_label].append(i)
				else:
					labels[class_label] = [i]

			#split into number of folds
			fold_indicies = [[] for x in range(self.folds)]

			# Want to represent class distribution in each set (stratified split)
			# Divide indicies list in chunks of size folds
			for key in labels:
				labels[key] = to_chunks(labels[key], self.folds)
				for i, fold_ids in enumerate(labels[key]):
					fold_indicies[i] += fold_ids

			fold_indicies = np.array([[int(x) for x in index_set] for index_set in fold_indicies])

			#take new copies of the original to use indicies from the original sets
			x, y = deepcopy(self.x), deepcopy(self.y)

			for i in list(range(self.folds)):
				test = np.array([(i) % self.folds]) # one fold is test set
				train = np.array([i for i in range(self.folds) if i not in [test]]) # remaining folds are training set ( not in [test, val])

				test_idxs = np.concatenate(tuple([fold_indicies[i] for i in test]))
				train_idxs = np.concatenate(tuple([fold_indicies[i] for i in train]))

				self.x_train = x[train_idxs]
				self.y_train = y[train_idxs]
				self.x_test = x[test_idxs]
				self.y_test = y[test_idxs]

				self.test_idxs = test_idxs
				
				stop = self.__train_and_test_model()
				if stop:
					return

				self.current_fold += 1

		# test-train
		else:
			self.x_train = deepcopy(self.X_TRAIN)
			self.y_train = deepcopy(self.Y_TRAIN)
			self.x_test = deepcopy(self.X_TEST)
			self.y_test = deepcopy(self.Y_TEST)

			self.test_idxs = np.array(range(1, len(self.y_test) + 1)) / 10 #divide by 10 to distinguish from training/10-fold data

			# remove short sequences - store indicies for test data
			self.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params["sequence_length"])
			self.x_test, self.y_test, self.test_idxs = remove_short_idx(self.x_test, self.y_test, self.test_idxs, self.current_params["sequence_length"])

			self.__train_and_test_model()
		

	def __train_and_test_model(self):

		print("test, train set size (x):", self.x_test.shape, self.x_train.shape,)
		print("test, train set size (y):", self.y_test.shape, self.y_train.shape,)
		#Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
		self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
		
		#scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)

		#Output size - in future delete any cols for categorical which are all zero 
		model = self.__generate_model(self.x_train, self.y_train, self.current_params)

		if self.current_fold == 1:
			print(model.summary())

		return self.__train_model(model) #Returns TRUE if accuracy below threshold 
			

	def __train_model(self, model):
		"""run one fold and write up results"""
		print("		fold ", self.current_fold, "of", self.folds)
		metrics = self.metrics
		reset_states = ResetStatesCallback()

		start_train = time.time()
		h = model.fit(
			self.x_train, self.y_train, 
			batch_size=self.current_params["batch_size"], 
			epochs=self.current_params["epochs"],
			verbose=0, 
			shuffle=True,	
			callbacks=[reset_states])

		end_train = time.time()

		metrics["train_acc"] = h.history["acc"]

		start_test = time.time()
		__pred_Y = model.predict(self.x_test, batch_size=self.current_params["batch_size"])
		metrics["preds"] = [x[0] for x in __pred_Y]
		end_test = time.time()
		metrics["truth"] = self.y_test.flatten()
		metrics["categorical_preds"] = [np.round(x) for x in metrics["preds"]]
		metrics["fscore"] = f1_score(metrics["truth"], metrics["categorical_preds"])
		metrics["accuracy"] = accuracy_score(metrics["truth"], metrics["categorical_preds"])		
		metrics["experiment_id"] = self.experiment_id
		metrics["training_size"] = len(self.y_train)
		metrics["test_size"] = len(self.y_test)
		metrics["train_time"] = end_train - start_train
		metrics["test_time"] = end_test - start_test
		metrics["fold_id"] = self.current_fold
		metrics["test_idxs"] = self.test_idxs

		if not self.metrics_headers:
			#Create files and Write file headers
			os.mkdir(self.folder_name)
			self.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())
			self.metrics_file = open("{}/results.csv".format(self.folder_name), "w")
			self.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)
			self.metrics_writer.writeheader()

		#Write up metric results
		self.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))

		#Search type changes
		print("acc:", metrics["accuracy"])

		#make space in memory
		del model
		gc.collect()

		if self.thresholding:
			if metrics["accuracy"] < self.temp_min_threshold:
				return True
			else:
				self.accuracy_scores.append(metrics["accuracy"])

			#On last fold check if average accuracy > current threshold, update temporary minimum to smallest from folds accuracy
			if self.current_fold == self.folds:
				average_acc = sum(self.accuracy_scores) / len(self.accuracy_scores)
				print("average acc:", average_acc)
				if average_acc > self.min_threshold:
					self.temp_min_threshold = min(self.accuracy_scores)
					self.min_threshold = average_acc
					print("* * * NEW RECORD avg acc:", average_acc, "min acc:", self.temp_min_threshold)

		return False # Only return true to stop models running


	def run_experiments(self, num_experiments=100):		
		# GRID SEARCH 
		#Find total possible configurations from options
		self.total = 1 
		for key in self.original_h_params:
			self.total *= len(self.original_h_params[key])

		if self.search_algorithm == "grid":
			header_list = list(self.h_params.keys()) #Fixed keys list to loop in order
			countdown = len(self.h_params) - 1
			self.num_experiments = self.total
			print("grid search of ", self.total, "configurations...")
			self.loop_values(header_list, countdown)

		# RANDOM SEARCH 
		elif self.search_algorithm == "random":
			self.num_experiments = num_experiments
			print("random search of ", self.num_experiments, "configurations of a possible", self.total, "configurations")
			while(self.experiment_id <= self.num_experiments):
				self.run_one_experiment()
		else: 
			print("ERROR: search algorithm not recognised")

		# Experiments run - close data files
		print(self.experiment_id, " models run.")

		self.metrics_file.close()


	def loop_values(self, header_list, countdown):
		# loop through all possible configurations in original parameter dictionary
		# http://stackoverflow.com/questions/7186518/function-with-varying-number-of-for-loops-python
		if (countdown > 0):
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.loop_values(header_list, countdown - 1)
		else:	
			for i in self.original_h_params[header_list[countdown]]:
				self.current_params[header_list[countdown]] = i
				self.run_one_experiment()



def Increase_Snaphot_Experiment(Experiment):
	def __train_and_test_model(self):

		# Keep only every step-th data snapshot
		self.x_train = self.x_train[:,::self.current_params["step"]]
		self.x_train = self.x_train[:,::self.current_params["step"]]

		# Shuffle data 
		self.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])
		self.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])
		
		# scale data by test data mean and variance
		means, stdvs = get_mean_and_stdv(self.x_train)
		self.x_train = scale_array(self.x_train, means, stdvs)
		self.x_test = scale_array(self.x_test, means, stdvs)

		# Output size - in future delete any cols for categorical which are all zero 
		model = self.__generate_model(self.x_train, self.y_train, self.current_params)

		if self.current_fold == 1:
			print(model.summary())

		return self.__train_model(model) #Returns TRUE if accuracy below threshold 